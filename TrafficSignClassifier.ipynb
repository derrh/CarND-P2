{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Traffic Sign Classifier\n",
    "\n",
    "By Derrick Hathaway\n",
    "<br/>November 11, 2017\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "We will be training a model to classify traffice signs from the German [Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "We will start by loading our data set which has been subdivided into three subsets: a training set which will be used to train the model, a validation set that we will use to validate the progress of our training, and a test set. Once training is complete we will use the test set to determine the accuracy of our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "training_file = 'train.p'\n",
    "validation_file = 'valid.p'\n",
    "test_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(test_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "X_train_in, y_train = train['features'], train['labels'].astype(np.uint8)\n",
    "X_valid_in, y_valid = valid['features'], valid['labels'].astype(np.uint8)\n",
    "X_test_in, y_test = test['features'], test['labels'].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "In order to improve training performance, we will take certain steps to condition the input data.\n",
    "\n",
    "First we will do a histogram equalization on the image to improve contrast. This effectively spreads out the input data over a broader range.\n",
    "\n",
    "Next, we will adjust the input data so that the range for each pixel is `(-1.0, 1.0]`.\n",
    "\n",
    "Next we will shuffle the training data so that the features and labels are encountered by the optimizer in a random order. If the input data are ordered, this can make it difficult for our network to converge on an acceptable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def adjust(x):\n",
    "    return x.astype(np.float32) / 128 - 1.0\n",
    "\n",
    "X_train = adjust(X_train_in)\n",
    "X_valid = adjust(X_valid_in)\n",
    "X_test = adjust(X_test_in)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "\n",
    "Below is a random sample from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "sign_names = []\n",
    "with open('samples/signnames.csv', 'r') as f:\n",
    "    data = list(csv.reader(f))\n",
    "    for i, sign_name in data:\n",
    "        sign_names.append(sign_name)\n",
    "\n",
    "def sign_name(num):\n",
    "    sign_names[num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 34799\n",
      "Number of validation examples = 4410\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n",
      "2     2010\n",
      "1     1980\n",
      "13    1920\n",
      "12    1890\n",
      "38    1860\n",
      "10    1800\n",
      "4     1770\n",
      "5     1650\n",
      "25    1350\n",
      "9     1320\n",
      "7     1290\n",
      "8     1260\n",
      "3     1260\n",
      "11    1170\n",
      "18    1080\n",
      "35    1080\n",
      "17     990\n",
      "31     690\n",
      "14     690\n",
      "33     599\n",
      "15     540\n",
      "26     540\n",
      "28     480\n",
      "23     450\n",
      "30     390\n",
      "34     360\n",
      "6      360\n",
      "16     360\n",
      "22     330\n",
      "36     330\n",
      "40     300\n",
      "20     300\n",
      "21     270\n",
      "39     270\n",
      "24     240\n",
      "29     240\n",
      "32     210\n",
      "42     210\n",
      "41     210\n",
      "27     210\n",
      "37     180\n",
      "19     180\n",
      "0      180\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "n_train = len(y_train)\n",
    "n_validation = len(y_valid)\n",
    "n_test = len(y_test)\n",
    "image_shape = X_train[0].shape\n",
    "unique = np.unique(y_train)\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", len(unique))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "counts = pd.value_counts(pd.Series(y_train))\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Slippery road\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAACPCAYAAAARM4LLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCP4bAeS2J5aZcDAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f90a162d1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow((image+1.0) / 2.0)\n",
    "plt.axis('off')\n",
    "print(\"Label\", sign_names[y_train[index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameters\n",
    "\n",
    "These are a few of the levers we can turn to adjust the training performance. These include the batch size, the number of epochs, the dropout rate used during training, the mean and standard deviation of our initial random weights, and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout = 0.50\n",
    "\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "\n",
    "learn_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "I've created some helper functions to help make the structure of my model more readable. This was very useful during development because it allowed me to adjust the size of the layers and make other changes with confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weights(shape):\n",
    "    return tf.truncated_normal(shape=shape, mean=mu, stddev=sigma)\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(\n",
    "        x,\n",
    "        W,\n",
    "        strides=[1, strides, strides, 1],\n",
    "        padding='VALID')\n",
    "    \n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LeNet Model\n",
    "\n",
    "I used the LeNet-5 model from the CNN Lesson as a starting point. I found that increasing the k-size of the convolution layers, as well as the size of the fully connected layers improved training performance significantly. The road sign images are more complex than black and white handwrittend digits so more depth in the convolutional layers is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "weights = {\n",
    "    'conv0': tf.Variable(weights([5,5,3,8])),\n",
    "    'conv1': tf.Variable(weights([5,5,8,24])),\n",
    "    'fc1': tf.Variable(weights([600, 120])),\n",
    "    'fc2': tf.Variable(weights([120, 84])),\n",
    "    'fc3': tf.Variable(weights([84, 43])),\n",
    "}\n",
    "\n",
    "bias = {\n",
    "    'conv0': tf.Variable(tf.zeros(8)),\n",
    "    'conv1': tf.Variable(tf.zeros(24)),\n",
    "    'fc1': tf.Variable(tf.zeros(120)),\n",
    "    'fc2': tf.Variable(tf.zeros(84)),\n",
    "    'fc3': tf.Variable(tf.zeros(43)),\n",
    "}\n",
    "\n",
    "def LeNet(x, dropout):\n",
    "    conv_0 = conv2d(x, weights['conv0'], bias['conv0'])\n",
    "    conv_0 = maxpool2d(conv_0)\n",
    "    \n",
    "    conv_1 = conv2d(conv_0, weights['conv1'], bias['conv1'])\n",
    "    conv_1 = maxpool2d(conv_1)\n",
    "    \n",
    "    flat = flatten(conv_1)\n",
    "    \n",
    "    fc1 = tf.add(tf.matmul(flat, weights['fc1']), bias['fc1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    fc2 = tf.add(tf.matmul(fc1, weights['fc2']), bias['fc2'])\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, dropout)\n",
    "    \n",
    "    fc3 = tf.add(tf.matmul(fc2, weights['fc3']), bias['fc3'])\n",
    "    \n",
    "    return fc3\n",
    "\n",
    "# signs dataset consists of 32x32x3 images\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "# Classify over 43 road signs labeled 0-42\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "logits = LeNet(x, keep_prob)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learn_rate)\n",
    "train_op = opt.minimize(loss_op)\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Testing\n",
    "\n",
    "### Method for Improving Performance\n",
    "\n",
    "I started out with the basic LeNet model from the MNIST classifier I built for the CNN Lab. After adapting the shape of the network to account for the 3 color channels of the image I mostly just tried different values for various hyper parameters. Here are some of the changes I made to improve performance.\n",
    "\n",
    "- Adjusting the learning rate. I tried values ranging from `0.01` - `0.0001`. The faster learning rates resulted in sort of erratic results, converging to a fairly high success rate, but very unpredictably. The lower learning rate did not achive a higher success rate, and converged very slowly. In the end I settled on `0.001`.\n",
    "- Implementing dropout. I played around with different dropout rates before arriving at `50%`.\n",
    "- Increasing the depth of the convolution layers and the width of the fully connected layers coupled with dropout also seemed to have pretty big impact on classification performance.\n",
    "- Adjusting the batch size. Larger batch sizes seemed to perform better for me. And I finally settled on a batch size of 128.\n",
    "- I tried various numbers of epochs, ranging from 6 to 20. I found that anything more than 10, give my other parameters tended to diverge slightly. Even now, the model will often peak in performance by about the 7th or 8th epoch. My final selection was 10 epochs.\n",
    "\n",
    "Choosing these parameters felt sort of arbitrary. I just tried tuning each value independently to see how it affected performance. When I felt satisfied with one of the parameter's values, I would tune a different parameter. I also went back occasionally to see if changes to a later parameter effected a previously tuned parameter, tweaking it again to see if a previouly poor value might perform better under new conditions. Tuning these parameter seems like the sort of thing that a computer could perform much better at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 ...\n",
      "Validation loss = 0.765\n",
      "Validation accuracy = 0.787\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation loss = 0.416\n",
      "Validation accuracy = 0.883\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation loss = 0.339\n",
      "Validation accuracy = 0.909\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation loss = 0.290\n",
      "Validation accuracy = 0.919\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation loss = 0.281\n",
      "Validation accuracy = 0.925\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation loss = 0.257\n",
      "Validation accuracy = 0.933\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation loss = 0.261\n",
      "Validation accuracy = 0.934\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation loss = 0.248\n",
      "Validation accuracy = 0.937\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation loss = 0.247\n",
      "Validation accuracy = 0.940\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation loss = 0.270\n",
      "Validation accuracy = 0.939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def eval_data(X_data, y_data):\n",
    "    \"\"\"\n",
    "    Given a dataset as input returns the loss and accuracy.\n",
    "    \"\"\"\n",
    "    num_examples = len(y_data)\n",
    "    total_acc, total_loss = 0, 0\n",
    "    sess = tf.get_default_session()\n",
    "    for batch in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x = X_data[batch:batch + BATCH_SIZE]\n",
    "        batch_y = y_data[batch:batch + BATCH_SIZE]\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_acc += (acc * batch_x.shape[0])\n",
    "        total_loss += (loss * batch_x.shape[0])\n",
    "    return total_loss/num_examples, total_acc/num_examples\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_file = './train_model.ckpt'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(y_train)\n",
    "\n",
    "    # Train model\n",
    "    for i in range(EPOCHS):\n",
    "        for batch in range(0, num_examples, BATCH_SIZE):\n",
    "            batch_x = X_train[batch:batch+BATCH_SIZE]\n",
    "            batch_y = y_train[batch:batch+BATCH_SIZE]\n",
    "            loss = sess.run(train_op, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "\n",
    "        val_loss, val_acc = eval_data(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation loss = {:.3f}\".format(val_loss))\n",
    "        print(\"Validation accuracy = {:.3f}\".format(val_acc))\n",
    "        print()\n",
    "\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 0.336\n",
      "Test accuracy = 0.937\n"
     ]
    }
   ],
   "source": [
    "## Testing Against Provided Test Dataset\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)   # Evaluate on the test data\n",
    "    test_loss, test_acc = eval_data(X_test, y_test)\n",
    "    print(\"Test loss = {:.3f}\".format(test_loss))\n",
    "    print(\"Test accuracy = {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing Against New Images\n",
    "\n",
    "I have found a few images from the web of various German traffic signs. We will use the trained model to classify these signs. I have cropped and resized the images to match the expected input of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/nopassing.png 'file') Sign Classification | SoftMax Probabilities |\n",
       "| ------------------------------ | ----------- |\n",
       "| No passing | 1.00000 |\n",
       "| Dangerous curve to the left | 0.00000 |\n",
       "| No passing for vehicles over 3.5 metric tons | 0.00000 |\n",
       "| Dangerous curve to the right | 0.00000 |\n",
       "| End of no passing | 0.00000 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/stop.png 'file') Sign Classification | SoftMax Probabilities |\n",
       "| ------------------------------ | ----------- |\n",
       "| Stop | 0.99988 |\n",
       "| Speed limit (60km/h) | 0.00005 |\n",
       "| Speed limit (50km/h) | 0.00004 |\n",
       "| Speed limit (30km/h) | 0.00001 |\n",
       "| Speed limit (80km/h) | 0.00001 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/yield.png 'file') Sign Classification | SoftMax Probabilities |\n",
       "| ------------------------------ | ----------- |\n",
       "| Yield | 1.00000 |\n",
       "| No vehicles | 0.00000 |\n",
       "| No passing | 0.00000 |\n",
       "| Road work | 0.00000 |\n",
       "| Speed limit (60km/h) | 0.00000 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/direction.png 'file') Sign Classification | SoftMax Probabilities |\n",
       "| ------------------------------ | ----------- |\n",
       "| Keep right | 1.00000 |\n",
       "| Speed limit (60km/h) | 0.00000 |\n",
       "| Turn left ahead | 0.00000 |\n",
       "| Roundabout mandatory | 0.00000 |\n",
       "| End of all speed and passing limits | 0.00000 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/noentry.png 'file') Sign Classification | SoftMax Probabilities |\n",
       "| ------------------------------ | ----------- |\n",
       "| No entry | 0.99919 |\n",
       "| Stop | 0.00081 |\n",
       "| Road work | 0.00000 |\n",
       "| No passing | 0.00000 |\n",
       "| Speed limit (30km/h) | 0.00000 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/twenty.png 'file') Sign Classification | SoftMax Probabilities |\n",
       "| ------------------------------ | ----------- |\n",
       "| Speed limit (20km/h) | 0.88909 |\n",
       "| Speed limit (30km/h) | 0.10868 |\n",
       "| Speed limit (80km/h) | 0.00104 |\n",
       "| Speed limit (120km/h) | 0.00080 |\n",
       "| Speed limit (60km/h) | 0.00039 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/fifty.png 'file') Sign Classification | SoftMax Probabilities |\n",
       "| ------------------------------ | ----------- |\n",
       "| Speed limit (50km/h) | 1.00000 |\n",
       "| Speed limit (80km/h) | 0.00000 |\n",
       "| Speed limit (30km/h) | 0.00000 |\n",
       "| Speed limit (60km/h) | 0.00000 |\n",
       "| Speed limit (100km/h) | 0.00000 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "top_k = tf.nn.top_k(tf.nn.softmax(logits), k=5)\n",
    "\n",
    "def classify_image(filename):\n",
    "    file = mpimg.imread(filename)\n",
    "    file = file[:,:,:3]\n",
    "    file = np.expand_dims(file, axis=0)\n",
    "    file = (file * 255).astype(np.uint8)\n",
    "    file = adjust(file)\n",
    "    sess = tf.get_default_session()\n",
    "    result = sess.run(top_k, feed_dict={x: file, keep_prob: 1.0})\n",
    "    output = \"\"\"\n",
    "| ![filename]({} 'file') Sign Classification | SoftMax Probabilities |\n",
    "| ------------------------------ | ----------- |\n",
    "\"\"\"\n",
    "    for i in range(len(result.values[0])):\n",
    "        output += \"| {} | {:.5f} |\\n\".format(\n",
    "            sign_names[result.indices[0][i]],\n",
    "            result.values[0][i]\n",
    "        )\n",
    "    display(Markdown(output.format(filename)))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    classify_image('samples/nopassing.png')\n",
    "    classify_image('samples/stop.png')\n",
    "    classify_image('samples/yield.png')\n",
    "    classify_image('samples/direction.png')\n",
    "    classify_image('samples/noentry.png')\n",
    "    classify_image('samples/twenty.png')\n",
    "    classify_image('samples/fifty.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "For my \"real world\" tests I chose 7 images of signs I found online. I cropped and resized them and used the same `adjust` function I used to prepare the training data. The result is that the classifier correctly identified 7 of the 7 signs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
