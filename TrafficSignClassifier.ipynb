{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Traffic Sign Classifier\n",
    "\n",
    "By Derrick Hathaway\n",
    "<br/>November 11, 2017\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "We will be training a model to classify traffice signs from the German [Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "We will start by loading our data set which has been subdivided into three subsets: a training set which will be used to train the model, a validation set that we will use to validate the progress of our training, and a test set. Once training is complete we will use the test set to determine the accuracy of our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "training_file = 'train.p'\n",
    "validation_file = 'valid.p'\n",
    "test_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(test_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "X_train_in, y_train = train['features'], train['labels'].astype(np.uint8)\n",
    "X_valid_in, y_valid = valid['features'], valid['labels'].astype(np.uint8)\n",
    "X_test_in, y_test = test['features'], test['labels'].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "\n",
    "Below is a random sample from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 34799\n",
      "Number of validation examples = 4410\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "n_train = len(y_train)\n",
    "n_validation = len(y_valid)\n",
    "n_test = len(y_test)\n",
    "image_shape = X_train_in[0].shape\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "sign_names = []\n",
    "with open('samples/signnames.csv', 'r') as f:\n",
    "    data = list(csv.reader(f))\n",
    "    for i, sign_name in data:\n",
    "        sign_names.append(sign_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Keep right\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAACPCAYAAAARM4LLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7Dtuzil5LAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f44bc05fcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(X_train_in))\n",
    "image = X_train_in[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "print(\"Label\", sign_names[y_train[index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "In order to improve training performance, we will take certain steps to condition the input data. The first step is to adjust the input data so that the range for each pixel is `(-1.0, 1.0]`.\n",
    "\n",
    "Next we will shuffle the training data so that the features and labels are encountered by the optimizer in a random order. If the input data are ordered, this can make it difficult for our network to converge on an acceptable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust(x):\n",
    "    return (x.astype(np.float32) - 128) / 128\n",
    "\n",
    "X_train = adjust(X_train_in)\n",
    "X_valid = adjust(X_valid_in)\n",
    "X_test = adjust(X_test_in)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameters\n",
    "\n",
    "These are a few of the levers we can turn to adjust the training performance. These include the batch size, the number of epochs, the dropout rate used during training, the mean and standard deviation of our initial random weights, and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout = 0.50\n",
    "\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "\n",
    "learn_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "I've created some helper functions to help make the structure of my model more readable. This was very useful during development because it allowed me to adjust the size of the layers and make other changes with confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weights(shape):\n",
    "    return tf.truncated_normal(shape=shape, mean=mu, stddev=sigma)\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(\n",
    "        x,\n",
    "        W,\n",
    "        strides=[1, strides, strides, 1],\n",
    "        padding='VALID')\n",
    "    \n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LeNet Model\n",
    "\n",
    "I used the LeNet-5 model from the CNN Lesson as a starting point. I found that increasing the k-size of the convolution layers, as well as the size of the fully connected layers improved training performance significantly. The road sign images are more complex than black and white handwrittend digits so more depth in the convolutional layers is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "weights = {\n",
    "    'conv0': tf.Variable(weights([5,5,3,24])),\n",
    "    'conv1': tf.Variable(weights([5,5,24,64])),\n",
    "    'fc1': tf.Variable(weights([1600, 600])),\n",
    "    'fc2': tf.Variable(weights([600, 200])),\n",
    "    'fc3': tf.Variable(weights([200, 43])),\n",
    "}\n",
    "\n",
    "bias = {\n",
    "    'conv0': tf.Variable(tf.zeros(24)),\n",
    "    'conv1': tf.Variable(tf.zeros(64)),\n",
    "    'fc1': tf.Variable(tf.zeros(600)),\n",
    "    'fc2': tf.Variable(tf.zeros(200)),\n",
    "    'fc3': tf.Variable(tf.zeros(43)),\n",
    "}\n",
    "\n",
    "def LeNet(x, dropout):\n",
    "    conv_0 = conv2d(x, weights['conv0'], bias['conv0'])\n",
    "    conv_0 = maxpool2d(conv_0)\n",
    "    \n",
    "    conv_1 = conv2d(conv_0, weights['conv1'], bias['conv1'])\n",
    "    conv_1 = maxpool2d(conv_1)\n",
    "    \n",
    "    flat = flatten(conv_1)\n",
    "    \n",
    "    fc1 = tf.add(tf.matmul(flat, weights['fc1']), bias['fc1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    fc2 = tf.add(tf.matmul(fc1, weights['fc2']), bias['fc2'])\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, dropout)\n",
    "    \n",
    "    fc3 = tf.add(tf.matmul(fc2, weights['fc3']), bias['fc3'])\n",
    "    \n",
    "    return fc3\n",
    "\n",
    "# signs dataset consists of 32x32x3 images\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "# Classify over 43 road signs labeled 0-42\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "logits = LeNet(x, keep_prob)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learn_rate)\n",
    "train_op = opt.minimize(loss_op)\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Testing\n",
    "\n",
    "### Method for Improving Performance\n",
    "\n",
    "I started out with the basic LeNet model from the MNIST classifier I built for the CNN Lab. After adapting the shape of the network to account for the 3 color channels of the image I mostly just tried different values for various hyper parameters. Here are some of the changes I made to improve performance.\n",
    "\n",
    "- Adjusting the learning rate. I tried values ranging from `0.01` - `0.0001`. The faster learning rates resulted in sort of erratic results, converging to a fairly high success rate, but very unpredictably. The lower learning rate did not achive a higher success rate, and converged very slowly. In the end I settled on `0.001`.\n",
    "- Implementing dropout. I played around with different dropout rates before arriving at `50%`.\n",
    "- Increasing the depth of the convolution layers and the width of the fully connected layers coupled with dropout also seemed to have pretty big impact on classification performance.\n",
    "- Adjusting the batch size. Larger batch sizes seemed to perform better for me. And I finally settled on a batch size of 128.\n",
    "- I tried various numbers of epochs, ranging from 6 to 20. I found that anything more than 10, give my other parameters tended to diverge slightly. Even now, the model will often peak in performance by about the 7th or 8th epoch. My final selection was 10 epochs.\n",
    "\n",
    "Choosing these parameters felt sort of arbitrary. I just tried tuning each value independently to see how it affected performance. When I felt satisfied with one of the parameter's values, I would tune a different parameter. I also went back occasionally to see if changes to a later parameter effected a previously tuned parameter, tweaking it again to see if a previouly poor value might perform better under new conditions. Tuning these parameter seems like the sort of thing that a computer could perform much better at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 ...\n",
      "Validation loss = 0.494\n",
      "Validation accuracy = 0.851\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation loss = 0.299\n",
      "Validation accuracy = 0.915\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation loss = 0.245\n",
      "Validation accuracy = 0.934\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation loss = 0.207\n",
      "Validation accuracy = 0.940\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation loss = 0.205\n",
      "Validation accuracy = 0.944\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation loss = 0.188\n",
      "Validation accuracy = 0.954\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation loss = 0.206\n",
      "Validation accuracy = 0.945\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation loss = 0.191\n",
      "Validation accuracy = 0.952\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation loss = 0.175\n",
      "Validation accuracy = 0.955\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation loss = 0.186\n",
      "Validation accuracy = 0.959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def eval_data(X_data, y_data):\n",
    "    \"\"\"\n",
    "    Given a dataset as input returns the loss and accuracy.\n",
    "    \"\"\"\n",
    "    num_examples = len(y_data)\n",
    "    total_acc, total_loss = 0, 0\n",
    "    sess = tf.get_default_session()\n",
    "    for batch in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x = X_data[batch:batch + BATCH_SIZE]\n",
    "        batch_y = y_data[batch:batch + BATCH_SIZE]\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_acc += (acc * batch_x.shape[0])\n",
    "        total_loss += (loss * batch_x.shape[0])\n",
    "    return total_loss/num_examples, total_acc/num_examples\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_file = './train_model.ckpt'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(y_train)\n",
    "\n",
    "    # Train model\n",
    "    for i in range(EPOCHS):\n",
    "        for batch in range(0, num_examples, BATCH_SIZE):\n",
    "            batch_x = X_train[batch:batch+BATCH_SIZE]\n",
    "            batch_y = y_train[batch:batch+BATCH_SIZE]\n",
    "            loss = sess.run(train_op, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "\n",
    "        val_loss, val_acc = eval_data(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation loss = {:.3f}\".format(val_loss))\n",
    "        print(\"Validation accuracy = {:.3f}\".format(val_acc))\n",
    "        print()\n",
    "\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 0.290\n",
      "Test accuracy = 0.949\n"
     ]
    }
   ],
   "source": [
    "## Testing Against Provided Test Dataset\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)   # Evaluate on the test data\n",
    "    test_loss, test_acc = eval_data(X_test, y_test)\n",
    "    print(\"Test loss = {:.3f}\".format(test_loss))\n",
    "    print(\"Test accuracy = {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing Against New Images\n",
    "\n",
    "I have found a few images from the web of various German traffic signs. We will use the trained model to classify these signs. I have cropped and resized the images to match the expected input of the network.\n",
    "\n",
    "| Image                                        | Name       |\n",
    "| -------------------------------------------- | ---------- |\n",
    "| ![stop](samples/stop.png \"Stop\")             | Stop       |\n",
    "| ![stop](samples/yield.png \"Yield\")           | Yield      |\n",
    "| ![stop](samples/prohibited.png \"Prohibited\") | Prohibited |\n",
    "| ![stop](samples/nopassing.png \"No Passing\")  | No Passing |\n",
    "| ![stop](samples/direction.png \"Direction\")   | Direction  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/nopassing.png 'file') Sign Classification | Probability |\n",
       "| ------------------------------ | ----------- |\n",
       "| No passing | 1.00 |\n",
       "| No passing for vehicles over 3.5 metric tons | 0.00 |\n",
       "| End of no passing | 0.00 |\n",
       "| Dangerous curve to the left | 0.00 |\n",
       "| Vehicles over 3.5 metric tons prohibited | 0.00 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/stop.png 'file') Sign Classification | Probability |\n",
       "| ------------------------------ | ----------- |\n",
       "| Stop | 0.64 |\n",
       "| No passing | 0.26 |\n",
       "| Priority road | 0.10 |\n",
       "| Yield | 0.01 |\n",
       "| No passing for vehicles over 3.5 metric tons | 0.00 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure why the stop sign classification failed. I suspect converting the images to a different color space would have helped with this classification.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/yield.png 'file') Sign Classification | Probability |\n",
       "| ------------------------------ | ----------- |\n",
       "| Yield | 1.00 |\n",
       "| No passing for vehicles over 3.5 metric tons | 0.00 |\n",
       "| Speed limit (30km/h) | 0.00 |\n",
       "| Speed limit (80km/h) | 0.00 |\n",
       "| No vehicles | 0.00 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/direction.png 'file') Sign Classification | Probability |\n",
       "| ------------------------------ | ----------- |\n",
       "| Keep right | 1.00 |\n",
       "| Roundabout mandatory | 0.00 |\n",
       "| Go straight or right | 0.00 |\n",
       "| Turn left ahead | 0.00 |\n",
       "| End of speed limit (80km/h) | 0.00 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "| ![filename](samples/prohibited.png 'file') Sign Classification | Probability |\n",
       "| ------------------------------ | ----------- |\n",
       "| Priority road | 0.83 |\n",
       "| No passing for vehicles over 3.5 metric tons | 0.09 |\n",
       "| Yield | 0.07 |\n",
       "| Road work | 0.01 |\n",
       "| Speed limit (80km/h) | 0.00 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "top_k = tf.nn.top_k(tf.nn.softmax(logits), k=5)\n",
    "\n",
    "def classify_image(filename):\n",
    "    file = mpimg.imread(filename)\n",
    "    file = file[:,:,:3]\n",
    "    file = np.expand_dims(file, axis=0)\n",
    "    sess = tf.get_default_session()\n",
    "    result = sess.run(top_k, feed_dict={x: file, keep_prob: 1.0})\n",
    "    output = \"\"\"\n",
    "| ![filename]({} 'file') Sign Classification | Probability |\n",
    "| ------------------------------ | ----------- |\n",
    "\"\"\"\n",
    "    for i in range(len(result.values[0])):\n",
    "        output += \"| {} | {:.2f} |\\n\".format(\n",
    "            sign_names[result.indices[0][i]],\n",
    "            result.values[0][i]\n",
    "        )\n",
    "    display(Markdown(output.format(filename)))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    classify_image('samples/nopassing.png')\n",
    "    \n",
    "    classify_image('samples/stop.png')\n",
    "    print(\"I'm not sure why the stop sign classification failed. I suspect converting the images to a different color space would have helped with this classification.\")\n",
    "    print(\"\")\n",
    "    \n",
    "    classify_image('samples/yield.png')\n",
    "    classify_image('samples/direction.png')\n",
    "    classify_image('samples/prohibited.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "For my \"real world\" tests the model performed at 60% efficiency. This is less that I would have expected. In hindsight using a different color space would likely have yielded better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
